# Awesome-Embodied-World-Model
Awesome paper list and repos of the paper [A survey of embodied world models](https://www.researchgate.net/publication/395713824_A_Survey_of_Embodied_World_Models).

## Architectures of Embodied World Models

### Video Generation Models
- **Genie**: Generative Interactive Environments. **`ICML 2024`** [[Paper](https://arxiv.org/abs/2402.15391)]
- **Sora**: Creating video from text. **`OpenAI 2024`** [[Website](https://openai.com/sora)]
- **Open-Sora**: Democratizing efficient video production for all. **`arXiv 2024`** [[Paper](https://arxiv.org/abs/2412.20404)] [[Code](https://github.com/hpcaitech/Open-Sora)]
- **Genie 2**: A large‚Äêscale foundation world model. **`DeepMind 2024`** [[Blog](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)]
- **iVideoGPT**: Interactive videogpts are scalable world models. **`NeurIPS 2024`** [[Paper](https://arxiv.org/abs/2405.15223)] [[Code](https://github.com/thuml/iVideoGPT)]
- **NOVA**: Autoregressive video generation without vector quantization. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2412.14169)] [[Code](https://github.com/baaivision/NOVA)]
- **Lumos-1**: On autoregressive video generation from a unified model perspective. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.08801)]
- **MAGI-1**: Autoregressive Video Generation at Scale. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.13211)]
- **Video-GPT**: Video-GPT via Next Clip Diffusion. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.12489)] [[Code](https://github.com/wilson1yan/VideoGPT)]
- **CogVideoX**: Text-to-video diffusion models with an expert transformer. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2408.06072)] [[Code](https://github.com/zai-org/CogVideo)]
- **Vid2World**: Crafting Video Diffusion Models to Interactive World Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.14357)]
- **Wan**: Open and Advanced Large-Scale Video Generative Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2503.20314)] [[Code](https://github.com/Wan-Video/Wan2.1)]

- **Cosmos**: World foundation model platform for physical AI. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2501.03575)] [[Code](https://github.com/NVIDIA/Cosmos)]



### 3D Reconstruction-enhanced Models
- **Spmem**: Video World Models with Long-term Spatial Memory. **`arXiv 2025`** [[Paper](hhttps://arxiv.org/pdf/2506.05284)] [[Project Page](https://spmem.github.io/)]
- **Geodrive**: Geodrive: 3d geometry-informed driving world model with precise action control. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.22421v1)] [[Code](https://github.com/antonioo-c/GeoDrive)]
- **Drivedreamer4d**: Drivedreamer4d: World models are effective data machines for 4d driving scene representation. **`CVPR2025`** [[Paper](https://arxiv.org/abs/2410.13571)] [[Code](https://github.com/GigaAI-research/DriveDreamer4D)]
- **Recondreamer**: Recondreamer: Crafting world models for driving scene reconstruction via online restoration. **`CVPR2025`** [[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Ni_Recondreamer_Crafting_World_Models_for_Driving_Scene_Reconstruction_via_Online_CVPR_2025_paper.pdf)]  
- **VGGT**: Vggt: Visual geometry grounded transformer. **`CVPR2025`** [[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Vggt_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf)]  
- **DeepVerse**: DeepVerse: 4D Autoregressive Video Generation as a World Model. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.01103)]  
- **Geometry Forcing**: Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.07982)]  
- **UniFuture**: Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2503.13587)] [[Code](https://github.com/dk-liang/UniFuture)]  
- **Aether**: Aether: Geometric-aware unified world modeling. **`ICCV 2025`** [[Paper](https://arxiv.org/abs/2503.18945)][[Code](https://github.com/InternRobotics/Aether)] 
- **Geo4D**: Geo4d: Leveraging video generators for geometric 4d scene reconstruction. **` ICCV 2025 Highlight`** [[Paper](https://arxiv.org/abs/2504.07961)][[Code](https://github.com/jzr99/Geo4D)]
- **PosePilot**: PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth. **`IEEE/RSJ IROS 2025`** [[Paper](https://arxiv.org/abs/2505.01729)]  
- **UniScene**: Uniscene: Unified occupancy-centric driving scene generation. **`CVPR2025`** [[Paper](https://arxiv.org/abs/2412.05435)][[Code](https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation)]
- **WonderFree**: WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.20590)]  
- **GaussianWorld**: Gaussianworld: Gaussian world model for streaming 3d occupancy prediction. **`CVPR2025`** [[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zuo_GaussianWorld_Gaussian_World_Model_for_Streaming_3D_Occupancy_Prediction_CVPR_2025_paper.pdf)]  
- **DriveWorld**: Driveworld: 4d pre-trained scene understanding via world models for autonomous driving. **`CVPR2024`** [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.pdf)]  
- **Dist-4D**: Dist-4d: Disentangled spatiotemporal diffusion with metric depth for 4d driving scene generation. **`ICCV 2025`** [[Paper](https://arxiv.org/abs/2503.15208)][[Code](https://github.com/royalmelon0505/dist4d)]
- **TesserAct**: TesserAct: learning 4D embodied world models. **`ICCV 2025`** [[Paper](https://arxiv.org/abs/2504.20995)][[Code](https://github.com/UMass-Embodied-AGI/TesserAct)]
- **FlowDreamer**: FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.10075)] [[Project Page](https://sharinka0715.github.io/FlowDreamer/)]  
- **Geometry-aware 4D Video Generation for Robot Manipulation**: Geometry-aware 4D Video Generation for Robot Manipulation. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.01099)] [[Code](https://github.com/lzylucy/4dgen)]  
- **ORV**: ORV: 4D Occupancy-centric Robot Video Generation. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.03079)] [[Code](https://orangesodahub.github.io/ORV/)]  
- **3D Persistent Embodied World Models**: Learning 3D Persistent Embodied World Models. **`arXiv 2025`** [[Paper](https://export.arxiv.org/abs/2505.05495)]  
- **HunyuanWorld 1.0**: HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.21809)] [[Code](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0)] [[Project Page](https://3d.hunyuan.tencent.com/sceneTo3D)]
### Latent Space World Models
- **PlaNet**: Learning Latent Dynamics for Planning from Pixels. **`ICML 2019`** [[Paper](https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf)] [[Code](https://github.com/google-research/planet)] [[Blog](https://planetrl.github.io/)]
- **Dreamer**: Dream to Control: Learning Behaviors by Latent Imagination. **`ICLR 2020`** [[Paper](https://arxiv.org/pdf/1912.01603)] [[Code](https://github.com/google-research/dreamer)]
- **DreamerV2**: Mastering Atari with Discrete World Models. **`ICLR 2021`** [[Paper](https://arxiv.org/pdf/2010.02193)] [[Code](https://github.com/danijar/dreamerv2)]
- **DreamerV3**: Dream to Control: Learning Behaviors by Latent Imagination. **`Nature 2025`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/danijar/dreamerv3)]

- **I-JEPA**: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. **`ICCV 2023`** [[Paper](https://arxiv.org/pdf/2301.08243)] [[Code](https://github.com/facebookresearch/ijepa)]
- **V-JEPA**: Revisiting Feature Prediction for Learning Visual Representations from Video. **`TMLR 2024`** [[Paper](https://arxiv.org/pdf/2404.08471)] [[Code](https://github.com/facebookresearch/jepa)]
- **V-JEPA 2, V-JEPA 2-AC**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning. **`Meta 2024`** [[Paper](https://arxiv.org/pdf/2506.09985)] [[Code](https://github.com/facebookresearch/vjepa2)] [[Website](https://ai.meta.com/vjepa/)] [[Blog](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)]

- **TD-MPC**: Temporal Difference Learning for Model Predictive Control. **`ICML 2022`** [[Paper](https://arxiv.org/pdf/2203.04955)] [[Code](https://github.com/nicklashansen/tdmpc)] [[Website](https://www.nicklashansen.com/td-mpc/)]
- **TD-MPC-offline**: Finetuning Offline World Models in the Real World. **`CoRL 2023 Oral`** [[Paper](https://arxiv.org/pdf/2310.16029)] [[Code](https://github.com/yunhaif/fowm)] [[Website](https://www.yunhaifeng.com/FOWM/)]
- **TD-MPC2**: TD-MPC2: Scalable, Robust World Models for Continuous Control. **`ICLR 2024 Spotlight`** [[Paper](https://arxiv.org/pdf/2310.16828)] [[Code](https://github.com/nicklashansen/tdmpc2)] [[Website](https://www.tdmpc2.com/)]

## Training Paradigm of Embodied World Models

### Instruction-conditioned Training
- **Sora**: Sora: A review on background, technology, limitations, and opportunities of large vision models. **`arXiv 2024`** [[Paper](https://arxiv.org/pdf/2402.17177)] [[Code](https://github.com/lichao-sun/SoraReview)]
- **RoboDreamer**: Robodreamer: Learning compositional world models for robot imagination. **`ICML 2024`** [[Paper](https://arxiv.org/pdf/2402.17177)] [[Code](https://github.com/rainbow979/robodreamer)] [[Website](https://robovideo.github.io)]
- **Pandora**: Pandora: Towards general world model with natural language actions and video states. **`arXiv 2024`** [[Paper](https://arxiv.org/pdf/2406.09455?)] [[Code](https://github.com/maitrix-org/Pandora)] [[Website](https://world-model.maitrix.org)]
- **Cosmos**: Cosmos world foundation model platform for physical ai. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2501.03575)] [[Code](https://github.com/nvidia-cosmos/cosmos-predict1)] [[Website](https://www.nvidia.com/en-us/ai/cosmos/)]
### Action-conditioned Training
- **Vid2World**: Vid2World: Crafting Video Diffusion Models to Interactive World Models. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2501.03575)] [[Website](https://knightnemo.github.io/vid2world/)]
- **UWM**: Cosmos world foundation model platform for physical ai. **`ICML 2025`** [[Paper](https://arxiv.org/pdf/2504.02792?)] [[Code](https://github.com/WEIRDLabUW/unified-world-model)] [[Website](https://weirdlabuw.github.io/uwm/)]
- **Enverse-AC**: Enerverse-ac: Envisioning embodied environments with action condition. **`ICML 2025`** [[Paper](https://arxiv.org/pdf/2505.09723)] [[Code](https://github.com/AgibotTech/EnerVerse-AC)] [[Website](https://annaj2178.github.io/EnerverseAC.github.io/)]
- **FLARE**: FLARE: Robot learning with implicit world modeling. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2505.15659?)] [[Website](https://research.nvidia.com/labs/gear/flare/)]
### Physics-informed Training
- **RoboScape**: RoboScape: Physics-informed Embodied World Model. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2506.23135)] [[Code](https://github.com/tsinghua-fib-lab/RoboScape)]
- **TesserAct**: TesserAct: learning 4D embodied world models. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2504.20995)] [[Code](https://github.com/UMass-Embodied-AGI/TesserAct)] [[Website](https://tesseractworld.github.io)]
### Video-action Joint Training
- **HMA**: Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2502.04296)] [[Code](https://github.com/liruiw/HMA)] [[Website](https://liruiw.github.io/hma/)]
- **UVA**: Unified video action model. **`RSS 2025`** [[Paper](https://arxiv.org/pdf/2503.00200)] [[Code](https://github.com/ShuangLI59/unified_video_action)] [[Website](https://unified-video-action-model.github.io)]
- **WorldVLA**: WorldVLA: Towards Autoregressive Action World Model. **`DAMO 2025`** [[Paper](https://arxiv.org/pdf/2506.21539?)] [[Code](https://github.com/alibaba-damo-academy/WorldVLA)] 
### RL-based Training
- **RLVR-World**: RLVR-World: Training World Models with Reinforcement Learning **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2505.13934?)] [[Code](https://thuml.github.io/RLVR-World/)] [[Website](https://thuml.github.io/RLVR-World)]
  
## Applications of Embodied World Models 

### Offline Robotic Data Generation Engine

- **DreamGen**: DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.12705)] [[Code](http://github.com/nvidia/GR00T-dreams)] [[Website](https://research.nvidia.com/labs/gear/dreamgen/)]
- **RoboTransfer**: RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.23171)] [[Code](https://github.com/HorizonRobotics/RoboTransfer)] [[Website](https://horizonrobotics.github.io/robot_lab/robotransfer/)]
- **EnerVerse-AC**: EnerVerse-AC: Envisioning Embodied Environments with Action Condition **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.09723)] [[Code](https://github.com/AgibotTech/EnerVerse-AC)] [[Website](https://annaj2178.github.io/EnerverseAC.github.io/)]

### Environment Substitute for Reinforcement Learning

- **GenRL**: GenRL: Multimodal-foundation world models for generalization in embodied agents **`NeurIPS 2024`** [[Paper](https://arxiv.org/abs/2406.18043)] [[Code](https://github.com/mazpie/genrl)] [[Website](https://mazpie.github.io/genrl/)]
- **iVideoGPT**: iVideoGPT: Interactive VideoGPTs are Scalable World Models **`NeurIPS 2024`** [[Paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/7dbb5bfab324e3b86af9bd0df15498dd-Paper-Conference.pdf)] [[Code](https://github.com/thuml/iVideoGPT)] [[Website](https://thuml.github.io/iVideoGPT/)]
- **DreamerV3**: Dream to Control: Learning Behaviors by Latent Imagination. **`Nature 2025`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/danijar/dreamerv3)]

### Robotic Policy Evaluator
- **WorldEval**: WorldEval: World Model as Real-World Robot Policies Evaluator. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2505.19017)] [[Code](https://github.com/liyaxuanliyaxuan/Worldeval)] [[Website](https://worldeval.github.io/)]
- **EnerVerse-AC**: EnerVerse-AC: Envisioning Embodied Environments with Action Condition **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.09723)] [[Code](https://github.com/AgibotTech/EnerVerse-AC)] [[Website](https://annaj2178.github.io/EnerverseAC.github.io/)]
- **RoboScape**: EnerVerse-AC: Envisioning Embodied Environments with Action Condition **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.23135)] [[Code](https://github.com/tsinghua-fib-lab/RoboScape)]

### Action Planner as Embodied Agents
- **GPC**: Strengthening Generative Robot Policies through Predictive World Modeling **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2502.00622)] [[Website](https://computationalrobotics.seas.harvard.edu/GPC/)]
- **VPP**: Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations **`ICML 2025 Spotlight`** [[Paper](https://arxiv.org/pdf/2412.14803)] [[Code](https://github.com/roboterax/video-prediction-policy)] [[Website](https://video-prediction-policy.github.io/)]
- **V-JEPA 2-AC**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning. **`Meta 2024`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/facebookresearch/vjepa2)] [[Website](https://ai.meta.com/vjepa/)] [[Blog](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)]

## Benchmarks of Embodied World Models

### Generated Data Quality
- **VBench** : Comprehensive Benchmark Suite for Video Generative Models **`CVPR 2024 Highlight`** [[Paper]](https://arxiv.org/abs/2311.17982) [[Code]](https://github.com/Vchitect/VBench) [[Website]](https://vchitect.github.io/VBench-project/)
- **T2V-CompBench**: A Comprehensive Benchmark for Compositional Text-to-video Generation **`CVPR 2025`** [[Paper]](https://arxiv.org/pdf/2407.14505) [[Code]](https://github.com/KaiyueSun98/T2V-CompBench) [[Website]](https://t2v-compbench.github.io/)
- **VBench-2.0**: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness **`arXiv 2025`** [[Paper]](https://arxiv.org/abs/2503.21755) [[Code]](https://github.com/Vchitect/VBench)
- **VideoPhy**: Evaluating Physical Commonsense for Video Generation **`ICLR 2025 Poster`** [[Paper]](https://arxiv.org/abs/2406.03520) [[Code]](https://github.com/Hritikbansal/videophy)
- **VideoPhy 2:** Challenging Action-Centric Physical Commonsense Evaluation of Video Generation **`arXiv 2025`** [[Paper]](http://arxiv.org/abs/2503.06800) [[Code]](https://github.com/Hritikbansal/videophy/tree/main/VIDEOPHY2) [[Website]](https://videophy2.github.io/)
- **PhyGenBench**: Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation **`ICML 2025`** [[Paper]](https://arxiv.org/abs/2410.05363#) [[Code]](https://github.com/OpenGVLab/PhyGenBench) [[Website]](https://phygenbench123.github.io/)
- **WorldModelBench**: Judging Video Generation Models As World Models  **`arXiv 2025`** [[Paper]](https://arxiv.org/pdf/2502.20694) [[Code]](https://github.com/WorldModelBench-Team/WorldModelBench/tree/main?tab=readme-ov-file#evaluation) [[Website]](https://worldmodelbench-team.github.io/)
- **EWMBench**: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models  **`arXiv 2025`** [[Paper]](https://arxiv.org/abs/2505.09694) [[Code]](https://github.com/AgibotTech/EWMBench)

### End-to-end Manipulation Evaluation
- **DreamerV3**: Dream to Control: Learning Behaviors by Latent Imagination. **`Nature 2025`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/danijar/dreamerv3)]
- **V-JEPA 2**: Self-Supervised Video Models Enable Understanding, Prediction and Planning. **`Meta 2024`** [[Paper](https://arxiv.org/pdf/2506.09985)] [[Code](https://github.com/facebookresearch/vjepa2)] [[Website](https://ai.meta.com/vjepa/)] [[Blog](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)]
- **WorldSimBench**: Towards Video Generation Models as World Simulators  **`ICML 2025`** [[Paper]](https://iranqin.github.io/WorldSimBench.github.io/assets/WorldSimBenchmark.pdf) [[Website]](https://iranqin.github.io/WorldSimBench.github.io/)

### Evaluation Reliability towards Policy Model
- **EWM**: Evaluating Robot Policies in a World Model, **`arXiv 2025`** [[Paper]](https://arxiv.org/abs/2506.00613)
- **WorldEval**: World Model as Real-World Robot Policies Evaluator. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2505.19017)] [[Code](https://github.com/liyaxuanliyaxuan/Worldeval)] [[Website](https://worldeval.github.io/)]
- **RoboScape**: Physics-informed Embodied World Model. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2506.23135)] [[Code](https://github.com/tsinghua-fib-lab/RoboScape)]

### Data Scaling in Downstream Policy Model
- **DreamGen**: Unlocking Generalization in Robot Learning through Neural Trajectories **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.12705)] [[Code](http://github.com/nvidia/GR00T-dreams)] [[Website](https://research.nvidia.com/labs/gear/dreamgen/)]
- **RoboTransfer**: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.23171)] [[Code](https://github.com/HorizonRobotics/RoboTransfer)] [[Website](https://horizonrobotics.github.io/robot_lab/robotransfer/)]
- **GenSim**: Generating Robotic Simulation Tasks via Large Language Models  **`ICLR 2024 Spotlight`** [[Paper](https://arxiv.org/abs/2310.01361)] [[Code](https://github.com/liruiw/GenSim)]
- **WorldGPT**: Empowering LLM as Multimodal World Model  **`MM 2024`** [[Paper](https://arxiv.org/abs/2404.18202)] [[Code](https://github.com/DCDmllm/WorldGPT)]
- **Traj-LLM**: A New Exploration for Empowering Trajectory Prediction with Pre-trained Large Language Models  **`ICLR 2025 Poster`** [[Paper](https://arxiv.org/abs/2405.04909)] [[Code](https://github.com/TJU-IDVLab/Traj-LLM)]
- **RoboScape**: Physics-informed Embodied World Model  **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2506.23135)] [[Code](https://github.com/tsinghua-fib-lab/RoboScape)]



