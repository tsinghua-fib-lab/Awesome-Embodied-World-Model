# Awesome-Embodied-World-Model
Awesome paper list and repos of the paper "A comprehensive survey of embodied world models".

## Architectures of Embodied World Models

### Video Generation Models
- **Genie**: Generative Interactive Environments. **`ICML 2024`** [[Paper](https://arxiv.org/abs/2402.15391)]
- **Sora**: Creating video from text. **`OpenAI 2024`** [[Website](https://openai.com/sora)]
- **Open-Sora**: Democratizing efficient video production for all. **`arXiv 2024`** [[Paper](https://arxiv.org/abs/2412.20404)]
- **Genie 2**: A large‚Äêscale foundation world model. **`DeepMind 2024`** [[Blog](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)]
- **iVideoGPT**: Interactive videogpts are scalable world models. **`NeurIPS 2024`** [[Paper](https://arxiv.org/abs/2405.15223)]
- **NOVA**: Autoregressive video generation without vector quantization. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2412.14169)]
- **Lumos-1**: On autoregressive video generation from a unified model perspective. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.08801)]
- **MAGI-1**: Autoregressive Video Generation at Scale. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.13211)]
- **Video-GPT**: Video-GPT via Next Clip Diffusion. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.12489)]
- **CogVideoX**: Text-to-video diffusion models with an expert transformer. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2408.06072)]
- **Vid2World**: Crafting Video Diffusion Models to Interactive World Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.14357)]
- **Wan**: Open and Advanced Large-Scale Video Generative Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2503.20314)]

- **Cosmos**: World foundation model platform for physical AI. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2501.03575)]



### 3D Reconstruction-enhanced Models

### Latent Space World Models
- **PlaNet**: Learning Latent Dynamics for Planning from Pixels. **`ICML 2019`** [[Paper](https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf)] [[Code](https://github.com/google-research/planet)] [[Blog](https://planetrl.github.io/)]
- **Dreamer**: Dream to Control: Learning Behaviors by Latent Imagination. **`ICLR 2020`** [[Paper](https://arxiv.org/pdf/1912.01603)] [[Code](https://github.com/google-research/dreamer)]
- **DreamerV2**: Mastering Atari with Discrete World Models. **`ICLR 2021`** [[Paper](https://arxiv.org/pdf/2010.02193)] [[Code](https://github.com/danijar/dreamerv2)]
- **DreamerV3**: Dream to Control: Learning Behaviors by Latent Imagination. **`Nature 2025`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/danijar/dreamerv3)]

- **I-JEPA**: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. **`ICCV 2023`** [[Paper](https://arxiv.org/pdf/2301.08243)] [[Code](https://github.com/facebookresearch/ijepa)]
- **V-JEPA**: Revisiting Feature Prediction for Learning Visual Representations from Video. **`TMLR 2024`** [[Paper](https://arxiv.org/pdf/2404.08471)] [[Code](https://github.com/facebookresearch/jepa)]
- **V-JEPA 2, V-JEPA 2-AC**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning. **`Meta 2024`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/facebookresearch/vjepa2)] [[Website](https://ai.meta.com/vjepa/)] [[Blog](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)]

- **TD-MPC**: Temporal Difference Learning for Model Predictive Control. **`ICML 2022`** [[Paper](https://arxiv.org/pdf/2203.04955)] [[Code](https://github.com/nicklashansen/tdmpc)] [[Website](https://www.nicklashansen.com/td-mpc/)]
- **TD-MPC-offline**: Finetuning Offline World Models in the Real World. **`CoRL 2023 Oral`** [[Paper](https://arxiv.org/pdf/2310.16029)] [[Code](https://github.com/yunhaif/fowm)] [[Website](https://www.yunhaifeng.com/FOWM/)]
- **TD-MPC2**: TD-MPC2: Scalable, Robust World Models for Continuous Control. **`ICLR 2024 Spotlight`** [[Paper](https://arxiv.org/pdf/2310.16828)] [[Code](https://github.com/nicklashansen/tdmpc2)] [[Website](https://www.tdmpc2.com/)]

## Training Paradigm of Embodied World Models

### Instruction-conditioned Training

### Action-conditioned Training

### Physics-informed Training

### Video-action Joint Training

### RL-based Training

## Applications of Embodied World Models 

### Offline Robotic Data Generation Engine

- **DreamGen**: DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.12705)] [[Code](http://github.com/nvidia/GR00T-dreams)] [[Website](https://research.nvidia.com/labs/gear/dreamgen/)]
- **RoboTransfer**: RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.23171)] [[Code](https://github.com/HorizonRobotics/RoboTransfer)] [[Website](https://horizonrobotics.github.io/robot_lab/robotransfer/)]
- **EnerVerse-AC**: EnerVerse-AC: Envisioning Embodied Environments with Action Condition **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.09723)] [[Code](https://github.com/AgibotTech/EnerVerse-AC)] [[Website](https://annaj2178.github.io/EnerverseAC.github.io/)]

### Environment Substitute for Reinforcement Learning

- **GenRL**: GenRL: Multimodal-foundation world models for generalization in embodied agents **`NeurIPS 2024`** [[Paper](https://arxiv.org/abs/2406.18043)] [[Code](https://github.com/mazpie/genrl)] [[Website](https://mazpie.github.io/genrl/)]
- **iVideoGPT**: iVideoGPT: Interactive VideoGPTs are Scalable World Models **`NeurIPS 2024`** [[Paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/7dbb5bfab324e3b86af9bd0df15498dd-Paper-Conference.pdf)] [[Code](https://github.com/thuml/iVideoGPT)] [[Website](https://thuml.github.io/iVideoGPT/)]
- **DreamerV3**: Dream to Control: Learning Behaviors by Latent Imagination. **`Nature 2025`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/danijar/dreamerv3)]

### Robotic Policy Evaluator
- **WorldEval**: WorldEval: World Model as Real-World Robot Policies Evaluator. **`arXiv 2025`** [[Paper](https://arxiv.org/pdf/2505.19017)] [[Code](https://github.com/liyaxuanliyaxuan/Worldeval)] [[Website](https://worldeval.github.io/)]
- **EnerVerse-AC**: EnerVerse-AC: Envisioning Embodied Environments with Action Condition **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.09723)] [[Code](https://github.com/AgibotTech/EnerVerse-AC)] [[Website](https://annaj2178.github.io/EnerverseAC.github.io/)]
- **RoboScape**: EnerVerse-AC: Envisioning Embodied Environments with Action Condition **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.23135)] [[Code](https://github.com/tsinghua-fib-lab/RoboScape)]

### Action Planner as Embodied Agents
- **GPC**: Strengthening Generative Robot Policies through Predictive World Modeling **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2502.00622)] [[Website](https://computationalrobotics.seas.harvard.edu/GPC/)]
- **VPP**: Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations**`ICML 2025 Spotlight`** [[Paper](https://arxiv.org/pdf/2412.14803)] [[Code](https://github.com/roboterax/video-prediction-policy)] [[Website](https://video-prediction-policy.github.io/)]
- **V-JEPA 2-AC**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning. **`Meta 2024`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/facebookresearch/vjepa2)] [[Website](https://ai.meta.com/vjepa/)] [[Blog](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)]

## Benchmarks of Embodied World Models

### Generated Data Quality

### End-to-end Manipulation Evaluation

### Evaluation Reliability towards Policy Model

### Data Scaling in Downstream Policy Model



