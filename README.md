# Awesome-Embodied-World-Model
Awesome paper list and repos of the paper "A comprehensive survey of embodied world models".

## Architectures of Embodied World Models

### Video Generation Models
- **Genie**: Generative Interactive Environments. **`ICML 2024`** [[Paper](https://arxiv.org/abs/2402.15391)]
- **Sora**: Creating video from text. **`OpenAI 2024`** [[Website](https://openai.com/sora)]
- **Open-Sora**: Democratizing efficient video production for all. **`arXiv 2024`** [[Paper](https://arxiv.org/abs/2412.20404)]
- **Genie 2**: A large‚Äêscale foundation world model. **`DeepMind 2024`** [[Blog](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)]
- **iVideoGPT**: Interactive videogpts are scalable world models. **`NeurIPS 2024`** [[Paper](https://arxiv.org/abs/2405.15223)]
- **NOVA**: Autoregressive video generation without vector quantization. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2412.14169)]
- **Lumos-1**: On autoregressive video generation from a unified model perspective. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.08801)]
- **MAGI-1**: Autoregressive Video Generation at Scale. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.13211)]
- **Video-GPT**: Video-GPT via Next Clip Diffusion. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.12489)]
- **CogVideoX**: Text-to-video diffusion models with an expert transformer. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2408.06072)]
- **Vid2World**: Crafting Video Diffusion Models to Interactive World Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.14357)]
- **Wan**: Open and Advanced Large-Scale Video Generative Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2503.20314)]

- **Cosmos**: World foundation model platform for physical AI. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2501.03575)]



### 3D Reconstruction-enhanced Models

### Latent Space World Models
-**PlaNet**: Learning Latent Dynamics for Planning from Pixels. **`ICML 2019`** [[Paper](https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf)] [[Code](https://github.com/google-research/planet)] [[Blog](https://planetrl.github.io/)]
-**Dreamer**: Dream to Control: Learning Behaviors by Latent Imagination. **`ICLR 2020`** [[Paper](https://arxiv.org/pdf/1912.01603)] [[Code](https://github.com/google-research/dreamer)]
-**DreamerV2**: Mastering Atari with Discrete World Models. **`ICLR 2021`** [[Paper](https://arxiv.org/pdf/2010.02193)] [[Code](https://github.com/danijar/dreamerv2)]
-**DreamerV3**: Dream to Control: Learning Behaviors by Latent Imagination. **`Nature 2025`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/danijar/dreamerv3)]

-**I-JEPA**: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. **`ICCV 2023`** [[Paper](https://arxiv.org/pdf/2301.08243)] [[Code](https://github.com/facebookresearch/ijepa)]
-**V-JEPA**: Revisiting Feature Prediction for Learning Visual Representations from Video. **`TMLR 2024`** [[Paper](https://arxiv.org/pdf/2404.08471)] [[Code](https://github.com/facebookresearch/jepa)]
-**V-JEPA 2, V-JEPA 2-AC**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning. **`Meta 2024`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/facebookresearch/vjepa2)] [[Website](https://ai.meta.com/vjepa/)] [[Blog](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)]

-**TD-MPC**: Temporal Difference Learning for Model Predictive Control. **`ICML 2022`** [[Paper](https://arxiv.org/pdf/2203.04955)] [[Code](https://github.com/nicklashansen/tdmpc)] [[Website](https://www.nicklashansen.com/td-mpc/)]
-**TD-MPC-offline**: Finetuning Offline World Models in the Real World. **`CoRL 2023 Oral`** [[Paper](https://arxiv.org/pdf/2310.16029)] [[Code](https://github.com/yunhaif/fowm)] [[Website](https://www.yunhaifeng.com/FOWM/)]
-**TD-MPC2**: TD-MPC2: Scalable, Robust World Models for Continuous Control. **`ICLR 2024 Spotlight`** [[Paper](https://arxiv.org/pdf/2310.16828)] [[Code](https://github.com/nicklashansen/tdmpc2)] [[Website](https://www.tdmpc2.com/)]

## Training Paradigm of Embodied World Models

### Instruction-conditioned Training

### Action-conditioned Training

### Physics-informed Training

### Video-action Joint Training

### RL-based Training

## Applications of Embodied World Models 

### Offline Robotic Data Generation Engine

### Environment Substitute for Reinforcement Learning

### Robotic Policy Evaluator

### Action Planner as Embodied Agents

## Benchmarks of Embodied World Models

### Generated Data Quality

### End-to-end Manipulation Evaluation

### Evaluation Reliability towards Policy Model

### Data Scaling in Downstream Policy Model




