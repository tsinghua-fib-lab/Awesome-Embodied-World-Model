# Awesome-Embodied-World-Model
Awesome paper list and repos of the paper "A comprehensive survey of embodied world models".

## Architectures of Embodied World Models

### Video Generation Models
- **Genie**: Generative Interactive Environments. **`ICML 2024`** [[Paper](https://arxiv.org/abs/2402.15391)]
- **Sora**: Creating video from text. **`OpenAI 2024`** [[Website](https://openai.com/sora)]
- **Open-Sora**: Democratizing efficient video production for all. **`arXiv 2024`** [[Paper](https://arxiv.org/abs/2412.20404)]
- **Genie 2**: A large‚Äêscale foundation world model. **`DeepMind 2024`** [[Blog](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)]
- **iVideoGPT**: Interactive videogpts are scalable world models. **`NeurIPS 2024`** [[Paper](https://arxiv.org/abs/2405.15223)]
- **NOVA**: Autoregressive video generation without vector quantization. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2412.14169)]
- **Lumos-1**: On autoregressive video generation from a unified model perspective. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.08801)]
- **MAGI-1**: Autoregressive Video Generation at Scale. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.13211)]
- **Video-GPT**: Video-GPT via Next Clip Diffusion. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.12489)]
- **CogVideoX**: Text-to-video diffusion models with an expert transformer. **`ICLR 2025`** [[Paper](https://arxiv.org/abs/2408.06072)]
- **Vid2World**: Crafting Video Diffusion Models to Interactive World Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.14357)]
- **Wan**: Open and Advanced Large-Scale Video Generative Models. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2503.20314)]

- **Cosmos**: World foundation model platform for physical AI. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2501.03575)]



### 3D Reconstruction-enhanced Models
- **spmem**: Video World Models with Long-term Spatial Memory. **`arXiv 2025`** [[Paper](hhttps://arxiv.org/pdf/2506.05284)] [[Project Page](https://spmem.github.io/)]
- **Geodrive**: Geodrive: 3d geometry-informed driving world model with precise action control. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.22421v1)] [[Code](https://github.com/antonioo-c/GeoDrive)]
- **Drivedreamer4d**: Drivedreamer4d: World models are effective data machines for 4d driving scene representation. **`CVPR2025`** [[Paper](https://arxiv.org/abs/2410.13571)] [[Code](https://github.com/GigaAI-research/DriveDreamer4D)]
- **Recondreamer**: Recondreamer: Crafting world models for driving scene reconstruction via online restoration. **`CVPR2025`** [[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Ni_Recondreamer_Crafting_World_Models_for_Driving_Scene_Reconstruction_via_Online_CVPR_2025_paper.pdf)]  
- **VGGT**: Vggt: Visual geometry grounded transformer. **`CVPR2025`** [[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Vggt_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf)]  
- **DeepVerse**: DeepVerse: 4D Autoregressive Video Generation as a World Model. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.01103)]  
- **Geometry Forcing**: Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.07982)]  
- **UniFuture**: Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2503.13587)] [[Code](https://github.com/dk-liang/UniFuture)]  
- **Aether**: Aether: Geometric-aware unified world modeling. **`ICCV 2025`** [[Paper](https://arxiv.org/abs/2503.18945)][[Code](https://github.com/InternRobotics/Aether)] 
- **Geo4D**: Geo4d: Leveraging video generators for geometric 4d scene reconstruction. **` ICCV 2025 Highlight`** [[Paper](https://arxiv.org/abs/2504.07961)][[Code](https://github.com/jzr99/Geo4D)]
- **PosePilot**: PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth. **`IEEE/RSJ IROS 2025`** [[Paper](https://arxiv.org/abs/2505.01729)]  
- **UniScene**: Uniscene: Unified occupancy-centric driving scene generation. **`CVPR2025`** [[Paper](https://arxiv.org/abs/2412.05435)][[Code](https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation)]
- **WonderFree**: WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.20590)]  
- **GaussianWorld**: Gaussianworld: Gaussian world model for streaming 3d occupancy prediction. **`CVPR2025`** [[Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zuo_GaussianWorld_Gaussian_World_Model_for_Streaming_3D_Occupancy_Prediction_CVPR_2025_paper.pdf)]  
- **DriveWorld**: Driveworld: 4d pre-trained scene understanding via world models for autonomous driving. **`CVPR2024`** [[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.pdf)]  
- **Dist-4D**: Dist-4d: Disentangled spatiotemporal diffusion with metric depth for 4d driving scene generation. **`ICCV 2025`** [[Paper](https://arxiv.org/abs/2503.15208)][[Code](https://github.com/royalmelon0505/dist4d)]
- **TesserAct**: TesserAct: learning 4D embodied world models. **`ICCV 2025`** [[Paper](https://arxiv.org/abs/2504.20995)][[Code](https://github.com/UMass-Embodied-AGI/TesserAct)]
- **FlowDreamer**: FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2505.10075)] [[Project Page](https://sharinka0715.github.io/FlowDreamer/)]  
- **Geometry-aware 4D Video Generation for Robot Manipulation**: Geometry-aware 4D Video Generation for Robot Manipulation. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.01099)] [[Code](https://github.com/lzylucy/4dgen)]  
- **ORV**: ORV: 4D Occupancy-centric Robot Video Generation. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2506.03079)] [[Code](https://orangesodahub.github.io/ORV/)]  
- **3D Persistent Embodied World Models**: Learning 3D Persistent Embodied World Models. **`arXiv 2025`** [[Paper](https://export.arxiv.org/abs/2505.05495)]  
- **HunyuanWorld 1.0**: HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels. **`arXiv 2025`** [[Paper](https://arxiv.org/abs/2507.21809)] [[Code](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0)] [[Project Page](https://3d.hunyuan.tencent.com/sceneTo3D)]
### Latent Space World Models
- **PlaNet**: Learning Latent Dynamics for Planning from Pixels. **`ICML 2019`** [[Paper](https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf)] [[Code](https://github.com/google-research/planet)] [[Blog](https://planetrl.github.io/)]
- **Dreamer**: Dream to Control: Learning Behaviors by Latent Imagination. **`ICLR 2020`** [[Paper](https://arxiv.org/pdf/1912.01603)] [[Code](https://github.com/google-research/dreamer)]
- **DreamerV2**: Mastering Atari with Discrete World Models. **`ICLR 2021`** [[Paper](https://arxiv.org/pdf/2010.02193)] [[Code](https://github.com/danijar/dreamerv2)]
- **DreamerV3**: Dream to Control: Learning Behaviors by Latent Imagination. **`Nature 2025`** [[Paper](https://www.nature.com/articles/s41586-025-08744-2)] [[Code](https://github.com/danijar/dreamerv3)]

- **I-JEPA**: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture. **`ICCV 2023`** [[Paper](https://arxiv.org/pdf/2301.08243)] [[Code](https://github.com/facebookresearch/ijepa)]
- **V-JEPA**: Revisiting Feature Prediction for Learning Visual Representations from Video. **`TMLR 2024`** [[Paper](https://arxiv.org/pdf/2404.08471)] [[Code](https://github.com/facebookresearch/jepa)]
- **V-JEPA 2, V-JEPA 2-AC**: V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning. **`Meta 2024`** [[Paper](https://arxiv.org/pdf/2506.09985)] [[Code](https://github.com/facebookresearch/vjepa2)] [[Website](https://ai.meta.com/vjepa/)] [[Blog](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/)]

- **TD-MPC**: Temporal Difference Learning for Model Predictive Control. **`ICML 2022`** [[Paper](https://arxiv.org/pdf/2203.04955)] [[Code](https://github.com/nicklashansen/tdmpc)] [[Website](https://www.nicklashansen.com/td-mpc/)]
- **TD-MPC-offline**: Finetuning Offline World Models in the Real World. **`CoRL 2023 Oral`** [[Paper](https://arxiv.org/pdf/2310.16029)] [[Code](https://github.com/yunhaif/fowm)] [[Website](https://www.yunhaifeng.com/FOWM/)]
- **TD-MPC2**: TD-MPC2: Scalable, Robust World Models for Continuous Control. **`ICLR 2024 Spotlight`** [[Paper](https://arxiv.org/pdf/2310.16828)] [[Code](https://github.com/nicklashansen/tdmpc2)] [[Website](https://www.tdmpc2.com/)]

## Training Paradigm of Embodied World Models

### Instruction-conditioned Training

### Action-conditioned Training

### Physics-informed Training

### Video-action Joint Training

### RL-based Training

## Applications of Embodied World Models 

### Offline Robotic Data Generation Engine

### Environment Substitute for Reinforcement Learning

### Robotic Policy Evaluator

### Action Planner as Embodied Agents

## Benchmarks of Embodied World Models

### Generated Data Quality

### End-to-end Manipulation Evaluation

### Evaluation Reliability towards Policy Model

### Data Scaling in Downstream Policy Model




